{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment with CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLn1uqWGHSjL+ar4rH/uZz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmayutrisna/Sentiment-Analysis-CNN/blob/main/Sentiment_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ykkxb8qU-_"
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from gensim import models\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGrYi3DEtvWR",
        "outputId": "a9abd48b-f31e-46a0-b29f-8f75cb261cfc"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJ0GUoSwIaL"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/imdb_labelled.tsv\", header = None, delimiter='\\t')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1BKl1ADLeuaw",
        "outputId": "e57e313f-0137-4ec8-c1a8-17dc65b6a238"
      },
      "source": [
        "data.columns = ['Text', 'Label']\n",
        "data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>I just got bored watching Jessice Lange take h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>In a word, it is embarrassing.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>Exceptionally bad!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>All in all its an insult to one's intelligence...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>748 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Text  Label\n",
              "0    A very, very, very slow-moving, aimless movie ...      0\n",
              "1    Not sure who was more lost - the flat characte...      0\n",
              "2    Attempting artiness with black & white and cle...      0\n",
              "3         Very little music or anything to speak of.        0\n",
              "4    The best scene in the movie was when Gerardo i...      1\n",
              "..                                                 ...    ...\n",
              "743  I just got bored watching Jessice Lange take h...      0\n",
              "744  Unfortunately, any virtue in this film's produ...      0\n",
              "745                   In a word, it is embarrassing.        0\n",
              "746                               Exceptionally bad!        0\n",
              "747  All in all its an insult to one's intelligence...      0\n",
              "\n",
              "[748 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ZvXnFGeyiH",
        "outputId": "cfeaa591-e6c5-453f-c2c9-f49b315f3bda"
      },
      "source": [
        "data.Label.value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    386\n",
              "0    362\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9MislMRe75r"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAyIQI9xfAGb"
      },
      "source": [
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UloHuIYOfDkK"
      },
      "source": [
        "data['Text_Clean'] = data['Text'].apply(lambda x: remove_punct(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YMJWSVBfH-J",
        "outputId": "b6ce9f3c-78da-4ab6-a417-c8defea6ed85"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "tokens = [word_tokenize(sen) for sen in data.Text_Clean]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8U2JFW2fjte"
      },
      "source": [
        "def lower_token(tokens): \n",
        "    return [w.lower() for w in tokens]    \n",
        "    \n",
        "lower_tokens = [lower_token(token) for token in tokens]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCLATxe-flxf",
        "outputId": "2551da76-6cc0-4872-a99b-2056f146cc14"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "def removeStopWords(tokens): \n",
        "    return [word for word in tokens if word not in stoplist]\n",
        "filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
        "data['Text_Final'] = [' '.join(sen) for sen in filtered_words]\n",
        "data['tokens'] = filtered_words"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QG_EhTU4f0zr",
        "outputId": "bc5a65d5-b83d-420a-cead-bf8b7450c978"
      },
      "source": [
        "# add one hot encoder columns to dataframe\n",
        "pos = []\n",
        "neg = []\n",
        "for l in data.Label:\n",
        "    if l == 0:\n",
        "        pos.append(0)\n",
        "        neg.append(1)\n",
        "    elif l == 1:\n",
        "        pos.append(1)\n",
        "        neg.append(0)\n",
        "data['Pos']= pos\n",
        "data['Neg']= neg\n",
        "\n",
        "data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]\n",
        "data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text_Final</th>\n",
              "      <th>tokens</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>slowmoving aimless movie distressed drifting y...</td>\n",
              "      <td>[slowmoving, aimless, movie, distressed, drift...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sure lost flat characters audience nearly half...</td>\n",
              "      <td>[sure, lost, flat, characters, audience, nearl...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>attempting artiness black white clever camera ...</td>\n",
              "      <td>[attempting, artiness, black, white, clever, c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>little music anything speak</td>\n",
              "      <td>[little, music, anything, speak]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best scene movie gerardo trying find song keep...</td>\n",
              "      <td>[best, scene, movie, gerardo, trying, find, so...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Text_Final  ... Neg\n",
              "0  slowmoving aimless movie distressed drifting y...  ...   1\n",
              "1  sure lost flat characters audience nearly half...  ...   1\n",
              "2  attempting artiness black white clever camera ...  ...   1\n",
              "3                        little music anything speak  ...   1\n",
              "4  best scene movie gerardo trying find song keep...  ...   0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNnI_2Wle44F"
      },
      "source": [
        "Spliting Data into test and train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxk60db6f-it"
      },
      "source": [
        "#we will use 90% data for training and 10% data for testing\n",
        "data_train, data_test = train_test_split(data, \n",
        "                                         test_size=0.10, \n",
        "                                         random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmpipPStgGQF",
        "outputId": "0d39b3ca-775c-4774-86d8-3fecbe569828"
      },
      "source": [
        "#build training vocabulary and get maximum training sentence length and total number of word training data\n",
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7218 words total, with a vocabulary size of 2881\n",
            "Max sentence length is 789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bStERR5UgI5K",
        "outputId": "3832569e-4abd-4869-e10d-500fb91ed3ce"
      },
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580 words total, with a vocabulary size of 457\n",
            "Max sentence length is 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXYgwtu7gN0t"
      },
      "source": [
        "**Loading Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZlnaeWegR3x"
      },
      "source": [
        "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCsfLPgriW9q"
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1hQUfbEifO5"
      },
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtYATXJuiiA1"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh8v_mOaijtn"
      },
      "source": [
        "**Tokenize and Pad Sequence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq3vXVJhip_j",
        "outputId": "edff9f79-c3dc-4638-acc8-3909e79a652e"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2881 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zmTWeRiuVi"
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvjI9W_8iy-i",
        "outputId": "aa8ecc4a-de29-4a11-b79b-f2245df34068"
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2882, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mxbm5M-i1Hx"
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILNvpmAxi8dc"
      },
      "source": [
        "**Define CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGRPYTHei-_P"
      },
      "source": [
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1KUZsW2jB2A"
      },
      "source": [
        "label_names = ['Pos', 'Neg']"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnqqv78rjFUq"
      },
      "source": [
        "y_train = data_train[label_names].values\n",
        "x_train = train_cnn_data\n",
        "y_tr = y_train"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjIbeP0jLru",
        "outputId": "879626e8-c36f-46ce-92d9-071e912bedc3"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       ...,\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI2oFZjtjQQf",
        "outputId": "64b20355-209f-4bc4-8df7-cb4a7f3468ba"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  560, 1017, 1018],\n",
              "       [   0,    0,    0, ...,  561, 1019,    2],\n",
              "       [   0,    0,    0, ...,    0,    9,  562],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  279, 2876,  115],\n",
              "       [   0,    0,    0, ..., 2880,   16,  914],\n",
              "       [   0,    0,    0, ...,    1,  939,   36]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZK5nLa4jVNz",
        "outputId": "0d574fd1-1607-46f0-d4e6-949ac4659795"
      },
      "source": [
        "y_tr"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       ...,\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQC3ONUTjZ0f",
        "outputId": "003e4f21-1282-4a69-9e5e-663f50d385d0"
      },
      "source": [
        "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 300)      864600      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 49, 200)      120200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 48, 200)      180200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 47, 200)      240200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 46, 200)      300200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 45, 200)      360200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "                                                                 global_max_pooling1d_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,193,986\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 864,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNjxFo18jzvD"
      },
      "source": [
        "**Train CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz5rKmA6j1ri",
        "outputId": "13953dac-45cc-47ca-e28a-2264f05993ee"
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 34\n",
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "18/18 [==============================] - 32s 34ms/step - loss: 0.7380 - acc: 0.5699 - val_loss: 0.6276 - val_acc: 0.7059\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4925 - acc: 0.8422 - val_loss: 0.4767 - val_acc: 0.8088\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2897 - acc: 0.8868 - val_loss: 0.4217 - val_acc: 0.8235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Hu-YvSkagk",
        "outputId": "43deebb3-6b4c-49e9-e7ec-c8b394509114"
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 309ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOSZrlfBkfhc"
      },
      "source": [
        "labels = [1, 0]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rNQ92Nfkhoc"
      },
      "source": [
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEpheNv4ltJm",
        "outputId": "342e023c-b8de-4f14-96bd-3515d86a0f61"
      },
      "source": [
        "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7466666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1h_Ggc8lvvw",
        "outputId": "8dc76b3c-bd8b-40da-f317-ef9b83f6799d"
      },
      "source": [
        "data_test.Label.value_counts()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    44\n",
              "1    31\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}